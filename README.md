# A.U.R.A. FARM - Autonomous Utility Robotic Arm for farming 

Autonomous farming robot system with data-driven decision making, computer vision, and inverse kinematics.

## System Architecture

### Vision Pipeline

The system uses OpenCV for color-based detection and YOLOv8 for object-specific recognition. The camera feed is processed to detect objects by color (red, black, blue, etc.) or by label (apple, bottle, etc.). Detected objects are tracked with bounding boxes and centroids, which are converted from pixel coordinates to real-world table coordinates using a calibrated mapping.

### Data-Driven Task Generation

The system integrates with the AgroMonitoring API to fetch real-time agricultural data and automatically generates robot tasks based on threshold violations:
- **Weather conditions**: Temperature, humidity, wind, precipitation
- **Soil metrics**: Moisture, pH, nitrogen, phosphorus, potassium
- **Plant health**: NDVI, growth stage, water stress, disease risk

Tasks are generated automatically when parameters exceed thresholds (e.g., soil moisture <30%, temperature >30°C, NDVI <0.5, disease risk >30%). The decision engine evaluates multiple parameters simultaneously and creates a prioritized task queue.

### Coordinate System

The system uses a bird's-eye view camera setup. Pixel coordinates from the camera are mapped to table coordinates (meters) using calibration parameters stored in `calibration.json`. The calibration defines:
- Origin point (table center)
- Scale factors (meters per pixel)
- Arm base position
- Coordinate flip settings

### Inverse Kinematics

For a 4-DOF robot arm (currently using 3 servos), the system calculates joint angles from target positions:
- **Base (D5)**: Rotation toward target
- **Shoulder (D18)**: Upper arm pitch
- **Elbow (D22)**: Lower arm pitch
- **Wrist**: Optional (currently disabled)

Joint angles are converted to servo microsecond commands (900-2100 range) for MG996R servos.

### Task Execution Pipeline

When a task is generated by the decision engine, the system executes it through the following pipeline:

1. **Task Selection**: Highest priority task is selected from the generated task queue
2. **Computer Vision**: YOLOv8 object detection locates target objects (crops, irrigation points, etc.) in the camera feed
3. **Coordinate Transformation**: Pixel coordinates from YOLOv8 are converted to world coordinates using calibrated camera-to-table mapping
4. **Inverse Kinematics**: Target coordinates are converted to joint angles for the robotic arm
5. **Servo Control**: Joint angles are converted to servo microsecond commands and sent to ESP32
6. **Execution**: ESP32 controls servos to move the arm to the target location

### Hardware Integration

The system generates servo commands that are sent to an ESP32 microcontroller. The ESP32 controls:
- 3x MG996R servos (Base D5, Shoulder D18, Elbow D22)

Servo control code is provided for Arduino IDE.

## Data Flow

1. **AgroMonitoring API** → Real-time agricultural data (weather, soil, plant health)
2. **Decision Engine** → Evaluates data against thresholds → Generates prioritized tasks
3. **Task Selection** → Highest priority task selected for execution
4. **Camera** → YOLOv8 object detection → Pixel coordinates of target
5. **Calibration** → Pixel to table coordinate conversion
6. **Kinematics** → Table coordinates → Joint angles → Servo commands
7. **ESP32** → Servo control → Arm movement → Task execution

## File Structure

- `main_sim.py`: Main loop integrating vision, task execution, and kinematics
- `detect.py`: OpenCV/YOLO object detection
- `kinematics.py`: Coordinate conversion and inverse kinematics
- `calibrate.py`: Interactive camera calibration tool
- `esp32_control.py`: ESP32 serial communication and servo control
- `frontend/`: React dashboard with task generation and monitoring
- `esp32_servo_control/`: ESP32 Arduino code for servo control
